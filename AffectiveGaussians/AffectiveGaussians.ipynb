{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "affective_gaussians",
   "display_name": "affective_gaussians"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Affective Gaussians\n",
    "Importing packages and support files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/AG_support')\n",
    "from AG_support.Text_processing import *\n",
    "import torchtext"
   ]
  },
  {
   "source": [
    "Defining paths and hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"C:\\\\Users\\\\fast\\\\camilo\\\\affective_gaussians\\\\data\\\\training_data\" \n",
    "vocab_file_path = \"C:\\\\Users\\\\fast\\\\camilo\\\\affective_gaussians\\\\data\\\\vocabulary.txt\" # if the file does not exist - it will be created\n",
    "output_folder_path = \"C:\\\\Users\\\\fast\\\\camilo\\\\affective_gaussians\\\\data\\\\output_data\" \n",
    "\n",
    "# Hyper-parameters\n",
    "half_window_size = 5  # (one sided)\n",
    "input_dim = 100\n",
    "h_dim = 100  # the number of components in the first hidden layers\n",
    "z_dim = 100  # the number of dimensions of the latent vectors\n",
    "learning_rate = 0.0075  # learning rate\n",
    "subsampling_threshold = None\n",
    "nr_neg_samples = 10\n",
    "margin = 5.0  # margin in the hinge loss\n",
    "epochs = 1\n",
    "max_vocab_size = 10000\n",
    "batch_size = 500"
   ]
  },
  {
   "source": [
    "**Generating Vocabulary from text data**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating vocabulary...\n",
      "Vocabulary written to C:\\Users\\fast\\camilo\\affective_gaussians\\data\\vocabulary.txt\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = Vocabulary(max_size=max_vocab_size, min_count=1)\n",
    "tokenizer = torchtext.data.get_tokenizer(\"basic_english\")\n",
    "data_iterator = TextDataIterator(train_data_path,tokenizer) #with default tokenizer from torchtext (basic_english)\n",
    "vocab.create(data_iterator,vocab_file_path)"
   ]
  },
  {
   "source": [
    "Or...\n",
    "\n",
    "**Loading previously generated vocabulary**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary loaded from C:\\Users\\fast\\camilo\\affective_gaussians\\data\\vocabulary.txt\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(max_size=max_vocab_size, min_count=1)\n",
    "vocab.load(vocab_file_path)"
   ]
  },
  {
   "source": [
    "Defining optimiser from [pytorch](https://pytorch.org/docs/stable/optim.html#algorithms). Also, [some theory on gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params,lr=learning_rate,betas)"
   ]
  },
  {
   "source": [
    "**Defining model architecture** using [pytorch](https://pytorch.org/docs/stable/nn.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ..."
   ]
  },
  {
   "source": [
    "**Training model**\n",
    "\n",
    "And saving train info in arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, epochs, etc"
   ]
  },
  {
   "source": [
    "**Visualisation in Tensorboard**\n",
    "\n",
    "Sources: [Pytorch-tensorboard tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html#) and [using tensorboard in notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\"\"\"\n",
    "writer = SummaryWriter('runs/exp1')\n",
    "writer.add_graph(model)\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
    "writer.add_figure()\n",
    "writer.add_hparams(hparam_dict, metric_dict)\n",
    "add_embedding()\n",
    "#etc\n",
    "writer.close()\"\"\"\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "**Evaluating from folder/eval.py**\n",
    "\n",
    "Evaluating representations (word similarity, entailment, etc)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run folder/eval.py"
   ]
  }
 ]
}